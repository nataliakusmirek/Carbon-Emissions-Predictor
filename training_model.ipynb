{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6339b8-e054-4c2a-98fd-cc7b4c54e9a0",
   "metadata": {},
   "source": [
    "## Training the model: XGBoost Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bd9ae6-aad2-4952-b012-95b234e19717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "253c273b-7f15-4b54-b978-edf16ebe62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_high = pd.read_csv('emissions_high_granularity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d198a82-4b3c-4cdc-9ee2-e137cac845af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the basics of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a493619-ec2c-4737-9518-59694d4b22bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>parent_entity</th>\n",
       "      <th>parent_type</th>\n",
       "      <th>reporting_entity</th>\n",
       "      <th>commodity</th>\n",
       "      <th>production_value</th>\n",
       "      <th>production_unit</th>\n",
       "      <th>product_emissions_MtCO2</th>\n",
       "      <th>flaring_emissions_MtCO2</th>\n",
       "      <th>venting_emissions_MtCO2</th>\n",
       "      <th>own_fuel_use_emissions_MtCO2</th>\n",
       "      <th>fugitive_methane_emissions_MtCO2e</th>\n",
       "      <th>fugitive_methane_emissions_MtCH4</th>\n",
       "      <th>total_operational_emissions_MtCO2e</th>\n",
       "      <th>total_emissions_MtCO2e</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962</td>\n",
       "      <td>Abu Dhabi National Oil Company</td>\n",
       "      <td>State-owned Entity</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>Oil &amp; NGL</td>\n",
       "      <td>0.9125</td>\n",
       "      <td>Million bbl/yr</td>\n",
       "      <td>0.338928</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018254</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.024957</td>\n",
       "      <td>0.363885</td>\n",
       "      <td>Abu Dhabi National Oil Company Annual Report 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1963</td>\n",
       "      <td>Abu Dhabi National Oil Company</td>\n",
       "      <td>State-owned Entity</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>Oil &amp; NGL</td>\n",
       "      <td>1.8250</td>\n",
       "      <td>Million bbl/yr</td>\n",
       "      <td>0.677855</td>\n",
       "      <td>0.010808</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036508</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.049914</td>\n",
       "      <td>0.727770</td>\n",
       "      <td>Abu Dhabi National Oil Company Annual Report 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1964</td>\n",
       "      <td>Abu Dhabi National Oil Company</td>\n",
       "      <td>State-owned Entity</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>Oil &amp; NGL</td>\n",
       "      <td>7.3000</td>\n",
       "      <td>Million bbl/yr</td>\n",
       "      <td>2.711422</td>\n",
       "      <td>0.043233</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146033</td>\n",
       "      <td>0.005215</td>\n",
       "      <td>0.199657</td>\n",
       "      <td>2.911079</td>\n",
       "      <td>Abu Dhabi National Oil Company Annual Report 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1965</td>\n",
       "      <td>Abu Dhabi National Oil Company</td>\n",
       "      <td>State-owned Entity</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>Oil &amp; NGL</td>\n",
       "      <td>10.9500</td>\n",
       "      <td>Million bbl/yr</td>\n",
       "      <td>4.067132</td>\n",
       "      <td>0.064849</td>\n",
       "      <td>0.015588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219049</td>\n",
       "      <td>0.007823</td>\n",
       "      <td>0.299486</td>\n",
       "      <td>4.366618</td>\n",
       "      <td>Abu Dhabi National Oil Company Annual Report 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1966</td>\n",
       "      <td>Abu Dhabi National Oil Company</td>\n",
       "      <td>State-owned Entity</td>\n",
       "      <td>Abu Dhabi</td>\n",
       "      <td>Oil &amp; NGL</td>\n",
       "      <td>13.5050</td>\n",
       "      <td>Million bbl/yr</td>\n",
       "      <td>5.016130</td>\n",
       "      <td>0.079980</td>\n",
       "      <td>0.019225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.270160</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.369366</td>\n",
       "      <td>5.385495</td>\n",
       "      <td>Abu Dhabi National Oil Company Annual Report 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                   parent_entity         parent_type reporting_entity  \\\n",
       "0  1962  Abu Dhabi National Oil Company  State-owned Entity        Abu Dhabi   \n",
       "1  1963  Abu Dhabi National Oil Company  State-owned Entity        Abu Dhabi   \n",
       "2  1964  Abu Dhabi National Oil Company  State-owned Entity        Abu Dhabi   \n",
       "3  1965  Abu Dhabi National Oil Company  State-owned Entity        Abu Dhabi   \n",
       "4  1966  Abu Dhabi National Oil Company  State-owned Entity        Abu Dhabi   \n",
       "\n",
       "   commodity  production_value production_unit  product_emissions_MtCO2  \\\n",
       "0  Oil & NGL            0.9125  Million bbl/yr                 0.338928   \n",
       "1  Oil & NGL            1.8250  Million bbl/yr                 0.677855   \n",
       "2  Oil & NGL            7.3000  Million bbl/yr                 2.711422   \n",
       "3  Oil & NGL           10.9500  Million bbl/yr                 4.067132   \n",
       "4  Oil & NGL           13.5050  Million bbl/yr                 5.016130   \n",
       "\n",
       "   flaring_emissions_MtCO2  venting_emissions_MtCO2  \\\n",
       "0                 0.005404                 0.001299   \n",
       "1                 0.010808                 0.002598   \n",
       "2                 0.043233                 0.010392   \n",
       "3                 0.064849                 0.015588   \n",
       "4                 0.079980                 0.019225   \n",
       "\n",
       "   own_fuel_use_emissions_MtCO2  fugitive_methane_emissions_MtCO2e  \\\n",
       "0                           0.0                           0.018254   \n",
       "1                           0.0                           0.036508   \n",
       "2                           0.0                           0.146033   \n",
       "3                           0.0                           0.219049   \n",
       "4                           0.0                           0.270160   \n",
       "\n",
       "   fugitive_methane_emissions_MtCH4  total_operational_emissions_MtCO2e  \\\n",
       "0                          0.000652                            0.024957   \n",
       "1                          0.001304                            0.049914   \n",
       "2                          0.005215                            0.199657   \n",
       "3                          0.007823                            0.299486   \n",
       "4                          0.009649                            0.369366   \n",
       "\n",
       "   total_emissions_MtCO2e                                             source  \n",
       "0                0.363885  Abu Dhabi National Oil Company Annual Report 1...  \n",
       "1                0.727770  Abu Dhabi National Oil Company Annual Report 1...  \n",
       "2                2.911079  Abu Dhabi National Oil Company Annual Report 1...  \n",
       "3                4.366618  Abu Dhabi National Oil Company Annual Report 1...  \n",
       "4                5.385495  Abu Dhabi National Oil Company Annual Report 1...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_high.shape # 15797 rows, 16 columns\n",
    "e_high.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83f77a58-2892-4f7f-81fc-bdddea8e43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "e_high = e_high.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "242b779c-8c5a-46f4-a145-8ac826d2c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values by year\n",
    "e_high = e_high.sort_values(by='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df3544e7-b722-4a26-9fc0-0ae06f14524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    parent_entity    parent_short\n",
      "15096         Westmoreland Mining    Westmoreland\n",
      "15098         Westmoreland Mining    Westmoreland\n",
      "15097         Westmoreland Mining    Westmoreland\n",
      "15101         Westmoreland Mining    Westmoreland\n",
      "15100         Westmoreland Mining    Westmoreland\n",
      "...                           ...             ...\n",
      "5543                 Devon Energy           Devon\n",
      "5542                 Devon Energy           Devon\n",
      "5341               Czech Republic  Czech Republic\n",
      "205    Alliance Resource Partners        Alliance\n",
      "15796                         YPF             YPF\n",
      "\n",
      "[15797 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# The names of each parent are too long for plotting, so we'll make a new column that just as shortened parent names for plot purposes\n",
    "short_names = {\n",
    "    'Westmoreland Mining': 'Westmoreland',\n",
    "    'CONSOL Energy': 'CONSOL',\n",
    "    'ExxonMobil': 'Exxon',\n",
    "    'Shell': 'Shell',\n",
    "    'Former Soviet Union': 'Soviet Union',\n",
    "    'Anglo American': 'Anglo American',\n",
    "    'Chevron': 'Chevron',\n",
    "    'Poland': 'Poland',\n",
    "    'BP': 'BP',\n",
    "    'ConocoPhillips': 'ConocoPhillips',\n",
    "    'National Iranian Oil Co.': 'NIOC',\n",
    "    'China (Cement)': 'China Cement',\n",
    "    'TotalEnergies': 'Total',\n",
    "    'Czechoslovakia': 'Czech.',\n",
    "    'Pemex': 'Pemex',\n",
    "    'Saudi Aramco': 'Saudi Aramco',\n",
    "    'Marathon Oil': 'Marathon Oil',\n",
    "    'Kiewit Mining Group': 'Kiewit Mining',\n",
    "    'China (Coal)': 'China Coal',\n",
    "    'Peabody Coal Group': 'Peabody Coal',\n",
    "    'Occidental Petroleum': 'Occidental',\n",
    "    'Kuwait Petroleum Corp.': 'Kuwait Pet.',\n",
    "    'Singareni Collieries': 'Singareni Collieries',\n",
    "    'British Coal Corporation': 'British Coal',\n",
    "    'North American Coal': 'NorthA Coal',\n",
    "    'Eni': 'Eni',\n",
    "    'Sasol': 'Sasol',\n",
    "    'BASF': 'BASF',\n",
    "    'Petrobras': 'Petrobras',\n",
    "    'BHP': 'BHP',\n",
    "    'ONGC India': 'ONGC',\n",
    "    'Hess Corporation': 'Hess',\n",
    "    'Pertamina': 'Pertamina',\n",
    "    'Rio Tinto': 'Rio Tinto',\n",
    "    'Egyptian General Petroleum': 'Egypt Pet.',\n",
    "    'Sonangol': 'Sonangol',\n",
    "    'Nigerian National Petroleum Corp.': 'NNPC',\n",
    "    'Sonatrach': 'Sonatrach',\n",
    "    'QatarEnergy': 'QatarEnergy',\n",
    "    'Petroleos de Venezuela': 'PDVSA',\n",
    "    'Iraq National Oil Company': 'Iraq NOC',\n",
    "    'Libya National Oil Corp.': 'Libya NOC',\n",
    "    'Abu Dhabi National Oil Company': 'ADNOC',\n",
    "    'North Korea': 'North Korea',\n",
    "    'Repsol': 'Repsol',\n",
    "    'RWE': 'RWE',\n",
    "    'Petroleum Development Oman': 'Oman Pet.',\n",
    "    'Syrian Petroleum': 'Syrian Pet.',\n",
    "    'Bapco Energies': 'Bapco',\n",
    "    'Cyprus AMAX Minerals': 'Cyprus AMAX',\n",
    "    'Woodside Energy': 'Woodside',\n",
    "    'Equinor': 'Equinor',\n",
    "    'Arch Resources': 'Arch Resources',\n",
    "    'Coal India': 'Coal India',\n",
    "    'Petronas': 'Petronas',\n",
    "    'Taiheiyo Cement': 'Taiheiyo',\n",
    "    'Vistra': 'Vistra',\n",
    "    'Alpha Metallurgical Resources': 'Alpha Met.',\n",
    "    'Murphy Oil': 'Murphy Oil',\n",
    "    'APA Corporation': 'APA',\n",
    "    'YPF': 'YPF',\n",
    "    'Ovintiv': 'Ovintiv',\n",
    "    'Ecopetrol': 'Ecopetrol',\n",
    "    'Suncor Energy': 'Suncor',\n",
    "    'CNOOC': 'CNOOC',\n",
    "    'Cenovus Energy': 'Cenovus',\n",
    "    'Devon Energy': 'Devon',\n",
    "    'American Consolidated Natural Resources': 'ACNR',\n",
    "    'Southwestern Energy': 'Southwestern',\n",
    "    'CNPC': 'CNPC',\n",
    "    'Gazprom': 'Gazprom',\n",
    "    'Canadian Natural Resources': 'CNRL',\n",
    "    'Heidelberg Materials': 'Heidelberg',\n",
    "    'Cemex': 'Cemex',\n",
    "    'Rosneft': 'Rosneft',\n",
    "    'Bumi Resources': 'Bumi',\n",
    "    'Holcim Group': 'Holcim',\n",
    "    'Santos': 'Santos',\n",
    "    'Coterra Energy': 'Coterra',\n",
    "    'PetroEcuador': 'PetroEcuador',\n",
    "    'PTTEP': 'PTTEP',\n",
    "    'EOG Resources': 'EOG',\n",
    "    'Kazakhstan': 'Kazakhstan',\n",
    "    'Ukraine': 'Ukraine',\n",
    "    'EQT Corporation': 'EQT',\n",
    "    'TurkmenGaz': 'TurkmenGaz',\n",
    "    'Russian Federation': 'Russia',\n",
    "    'Adaro Energy': 'Adaro',\n",
    "    'Czech Republic': 'Czech Republic',\n",
    "    'Slovakia': 'Slovakia',\n",
    "    'Chesapeake Energy': 'Chesapeake',\n",
    "    'SM Energy': 'SM Energy',\n",
    "    'Pioneer Natural Resources': 'Pioneer',\n",
    "    'UK Coal': 'UK Coal',\n",
    "    'Alliance Resource Partners': 'Alliance',\n",
    "    'Obsidian Energy': 'Obsidian',\n",
    "    'Lukoil': 'Lukoil',\n",
    "    'OMV Group': 'OMV',\n",
    "    'Banpu': 'Banpu',\n",
    "    'Exxaro Resources Ltd': 'Exxaro',\n",
    "    'Glencore': 'Glencore',\n",
    "    'Teck Resources': 'Teck',\n",
    "    'Orlen': 'Orlen',\n",
    "    'Naftogaz': 'Naftogaz',\n",
    "    'Sinopec': 'Sinopec',\n",
    "    'Petoro': 'Petoro',\n",
    "    'Surgutneftegas': 'Surgut',\n",
    "    'Tullow Oil': 'Tullow',\n",
    "    'Wolverine Fuels': 'Wolverine',\n",
    "    'CRH': 'CRH',\n",
    "    'Novatek': 'Novatek',\n",
    "    'Inpex': 'Inpex',\n",
    "    'Continental Resources': 'Continental',\n",
    "    'Whitehaven Coal': 'Whitehaven',\n",
    "    'Vale': 'Vale',\n",
    "    'Cloud Peak': 'Cloud Peak',\n",
    "    'Tourmaline Oil': 'Tourmaline',\n",
    "    'Antero': 'Antero',\n",
    "    'Adani Enterprises': 'Adani',\n",
    "    'Navajo Transitional Energy Company': 'Navajo',\n",
    "    'CNX Resources': 'CNX',\n",
    "    'Seriti Resources': 'Seriti'\n",
    "}\n",
    "\n",
    "\n",
    "e_high['parent_short'] = e_high['parent_entity'].map(short_names)\n",
    "e_high['parent_short'] = e_high['parent_short'].fillna('Other')\n",
    "print(e_high[['parent_entity', 'parent_short']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121ce4b-06a1-4996-9e70-d06e75f999d9",
   "metadata": {},
   "source": [
    "### ML Model Pipeline starts here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096e80b-9811-4145-a36e-0509901c5718",
   "metadata": {},
   "source": [
    "#### Feature Engineering with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ee41b39-e4f1-45a8-b62f-8e5fa2974d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required columns are present.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target\n",
    "features = ['year', 'parent_entity', \n",
    "            'parent_type', 'commodity', \n",
    "            'product_emissions_MtCO2',\n",
    "            'flaring_emissions_MtCO2', 'venting_emissions_MtCO2',\n",
    "            'total_operational_emissions_MtCO2e']\n",
    "target = 'total_emissions_MtCO2e'\n",
    "\n",
    "# Check if all required columns are present (comment out later)\n",
    "required_columns = features + [target]\n",
    "missing_columns = [col for col in required_columns if col not in e_high.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    print(\"All required columns are present.\")\n",
    "\n",
    "# Split data into the features and targets defined\n",
    "X = e_high[features]\n",
    "y = e_high[target]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing for numeric and categorical features\n",
    "numeric_features = ['year', 'product_emissions_MtCO2', 'flaring_emissions_MtCO2', \n",
    "                     'venting_emissions_MtCO2', 'total_operational_emissions_MtCO2e']\n",
    "categorical_features = ['parent_entity', 'parent_type', 'commodity']\n",
    "\n",
    "# Standardize numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess the training and testing data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af5839-ddce-47f9-84f1-2a70d8983e54",
   "metadata": {},
   "source": [
    "#### Build the Neural Network with Tensorflow Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ba1ee77-3d32-4fc4-81b3-ce2ceeba61a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.64.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Downloading keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.3.5)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Downloading optree-0.12.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m798.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.16.2-cp311-cp311-macosx_12_0_arm64.whl (227.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.0/227.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.64.1-cp311-cp311-macosx_10_9_universal2.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.12.1-cp311-cp311-macosx_11_0_arm64.whl (283 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.64.1 h5py-3.11.0 keras-3.4.1 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.12.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.2 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2c405bbc-1eb2-4f99-a7df-26010283cb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in DataFrame: Index(['year', 'parent_entity', 'parent_type', 'reporting_entity', 'commodity',\n",
      "       'production_value', 'production_unit', 'product_emissions_MtCO2',\n",
      "       'flaring_emissions_MtCO2', 'venting_emissions_MtCO2',\n",
      "       'own_fuel_use_emissions_MtCO2', 'fugitive_methane_emissions_MtCO2e',\n",
      "       'fugitive_methane_emissions_MtCH4',\n",
      "       'total_operational_emissions_MtCO2e', 'total_emissions_MtCO2e',\n",
      "       'source', 'parent_short'],\n",
      "      dtype='object')\n",
      "All required columns are present.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 80184.4453 - mae: 81.6014 - val_loss: 39975.8398 - val_mae: 64.6910\n",
      "Epoch 2/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - loss: 24886.5078 - mae: 49.8249 - val_loss: 8233.5400 - val_mae: 29.1407\n",
      "Epoch 3/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 6148.9658 - mae: 25.9187 - val_loss: 1676.3706 - val_mae: 16.4281\n",
      "Epoch 4/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 1306.8154 - mae: 14.6957 - val_loss: 679.7532 - val_mae: 13.2569\n",
      "Epoch 5/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 532.7264 - mae: 11.9577 - val_loss: 449.0153 - val_mae: 10.8487\n",
      "Epoch 6/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - loss: 356.6356 - mae: 10.0139 - val_loss: 326.5858 - val_mae: 9.1961\n",
      "Epoch 7/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 260.5335 - mae: 8.4739 - val_loss: 255.4225 - val_mae: 7.7810\n",
      "Epoch 8/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - loss: 211.5579 - mae: 6.9179 - val_loss: 207.3311 - val_mae: 6.6474\n",
      "Epoch 9/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 160.1420 - mae: 6.0322 - val_loss: 182.4813 - val_mae: 6.0139\n",
      "Epoch 10/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 138.7491 - mae: 5.2620 - val_loss: 164.8001 - val_mae: 5.4046\n",
      "Epoch 11/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 151.7839 - mae: 4.7757 - val_loss: 149.3315 - val_mae: 4.9131\n",
      "Epoch 12/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 127.8876 - mae: 4.4597 - val_loss: 142.9090 - val_mae: 4.6932\n",
      "Epoch 13/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 104.1976 - mae: 4.1161 - val_loss: 128.7731 - val_mae: 4.3593\n",
      "Epoch 14/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - loss: 127.5969 - mae: 3.9993 - val_loss: 121.3230 - val_mae: 4.0917\n",
      "Epoch 15/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 96.0875 - mae: 3.5792 - val_loss: 112.6006 - val_mae: 4.0194\n",
      "Epoch 16/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 63.9837 - mae: 3.3399 - val_loss: 113.2863 - val_mae: 3.8865\n",
      "Epoch 17/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 103.2674 - mae: 3.4716 - val_loss: 99.6318 - val_mae: 3.6138\n",
      "Epoch 18/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 72.4066 - mae: 3.1019 - val_loss: 86.5927 - val_mae: 3.3255\n",
      "Epoch 19/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 73.2328 - mae: 2.9266 - val_loss: 81.4221 - val_mae: 3.2724\n",
      "Epoch 20/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - loss: 47.5005 - mae: 2.6871 - val_loss: 80.4716 - val_mae: 3.2332\n",
      "Epoch 21/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 623us/step - loss: 54.4128 - mae: 2.6537 - val_loss: 70.5398 - val_mae: 3.0570\n",
      "Epoch 22/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 62.1945 - mae: 2.7199 - val_loss: 59.7293 - val_mae: 2.9537\n",
      "Epoch 23/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 48.0213 - mae: 2.5253 - val_loss: 57.6426 - val_mae: 3.0304\n",
      "Epoch 24/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 27.5536 - mae: 2.2466 - val_loss: 48.4844 - val_mae: 2.6432\n",
      "Epoch 25/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 30.0090 - mae: 2.2131 - val_loss: 46.3791 - val_mae: 2.7584\n",
      "Epoch 26/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 24.1959 - mae: 2.0443 - val_loss: 41.4421 - val_mae: 2.6205\n",
      "Epoch 27/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 33.3577 - mae: 2.1666 - val_loss: 30.8618 - val_mae: 2.3867\n",
      "Epoch 28/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 19.7380 - mae: 1.8439 - val_loss: 28.2426 - val_mae: 2.1739\n",
      "Epoch 29/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 21.6330 - mae: 2.0000 - val_loss: 23.1254 - val_mae: 2.1587\n",
      "Epoch 30/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 17.8019 - mae: 1.7943 - val_loss: 21.4099 - val_mae: 2.2015\n",
      "Epoch 31/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 12.4972 - mae: 1.6951 - val_loss: 16.2027 - val_mae: 1.9282\n",
      "Epoch 32/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - loss: 12.0325 - mae: 1.6974 - val_loss: 14.3681 - val_mae: 1.9172\n",
      "Epoch 33/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 13.6738 - mae: 1.9849 - val_loss: 13.6275 - val_mae: 2.1000\n",
      "Epoch 34/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 11.6127 - mae: 1.7863 - val_loss: 26.4888 - val_mae: 3.6108\n",
      "Epoch 35/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 9.9956 - mae: 1.9922 - val_loss: 10.3408 - val_mae: 1.9035\n",
      "Epoch 36/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 5.8547 - mae: 1.4208 - val_loss: 10.3343 - val_mae: 2.1110\n",
      "Epoch 37/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 8.6180 - mae: 1.7174 - val_loss: 10.2437 - val_mae: 1.7475\n",
      "Epoch 38/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 4.8951 - mae: 1.3160 - val_loss: 7.1456 - val_mae: 1.6227\n",
      "Epoch 39/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - loss: 2.7928 - mae: 1.1053 - val_loss: 5.2843 - val_mae: 1.4290\n",
      "Epoch 40/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - loss: 10.2622 - mae: 1.4771 - val_loss: 8.8273 - val_mae: 2.1327\n",
      "Epoch 41/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 4.3152 - mae: 1.4660 - val_loss: 7.3359 - val_mae: 1.6702\n",
      "Epoch 42/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 13.8762 - mae: 1.9834 - val_loss: 7.1912 - val_mae: 1.7805\n",
      "Epoch 43/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - loss: 3.5245 - mae: 1.2231 - val_loss: 6.6294 - val_mae: 1.6197\n",
      "Epoch 44/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 18.6974 - mae: 2.2667 - val_loss: 6.7452 - val_mae: 1.7674\n",
      "Epoch 45/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 5.1520 - mae: 1.3477 - val_loss: 4.0162 - val_mae: 1.3579\n",
      "Epoch 46/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 1.6048 - mae: 0.8984 - val_loss: 4.2203 - val_mae: 1.3429\n",
      "Epoch 47/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - loss: 1.4023 - mae: 0.8220 - val_loss: 3.3797 - val_mae: 1.2820\n",
      "Epoch 48/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - loss: 1.2804 - mae: 0.8078 - val_loss: 4.7167 - val_mae: 1.3251\n",
      "Epoch 49/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 7.7652 - mae: 1.5664 - val_loss: 29.3680 - val_mae: 3.1160\n",
      "Epoch 50/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 34.6797 - mae: 2.9361 - val_loss: 3.3807 - val_mae: 1.2817\n",
      "Epoch 51/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 1.4041 - mae: 0.8520 - val_loss: 3.2030 - val_mae: 1.2084\n",
      "Epoch 52/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 1.8216 - mae: 0.8875 - val_loss: 2.8521 - val_mae: 1.1328\n",
      "Epoch 53/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - loss: 1.0628 - mae: 0.7320 - val_loss: 2.9006 - val_mae: 1.1628\n",
      "Epoch 54/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 1.0794 - mae: 0.7317 - val_loss: 2.3997 - val_mae: 1.0592\n",
      "Epoch 55/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 3.0590 - mae: 0.9384 - val_loss: 2.7365 - val_mae: 1.1601\n",
      "Epoch 56/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - loss: 1.6026 - mae: 0.8187 - val_loss: 5.9441 - val_mae: 1.6233\n",
      "Epoch 57/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 23.6301 - mae: 2.5712 - val_loss: 3.0713 - val_mae: 1.2258\n",
      "Epoch 58/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 2.2432 - mae: 0.9555 - val_loss: 4.4840 - val_mae: 1.5167\n",
      "Epoch 59/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - loss: 6.2365 - mae: 1.3083 - val_loss: 6.2991 - val_mae: 1.8913\n",
      "Epoch 60/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 77.1847 - mae: 3.5422 - val_loss: 8.9814 - val_mae: 2.2108\n",
      "Epoch 61/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 4.7888 - mae: 1.4188 - val_loss: 4.3115 - val_mae: 1.4322\n",
      "Epoch 62/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 3.0323 - mae: 1.0525 - val_loss: 23.5863 - val_mae: 2.3282\n",
      "Epoch 63/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 7.1967 - mae: 1.4838 - val_loss: 5.8919 - val_mae: 1.6830\n",
      "Epoch 64/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 1.8957 - mae: 0.9458 - val_loss: 2.8137 - val_mae: 1.0835\n",
      "Epoch 65/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 0.9350 - mae: 0.6695 - val_loss: 2.4465 - val_mae: 1.0570\n",
      "Epoch 66/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - loss: 1.4666 - mae: 0.7556 - val_loss: 2.6746 - val_mae: 1.0992\n",
      "Epoch 67/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 3.1587 - mae: 0.9256 - val_loss: 2.5478 - val_mae: 1.0785\n",
      "Epoch 68/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 1.2914 - mae: 0.7475 - val_loss: 4.4096 - val_mae: 1.4840\n",
      "Epoch 69/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 1.8328 - mae: 0.9265 - val_loss: 11.8615 - val_mae: 2.2982\n",
      "Epoch 70/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 74.4686 - mae: 4.2791 - val_loss: 3.5887 - val_mae: 1.3471\n",
      "Epoch 71/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 1.2449 - mae: 0.8244 - val_loss: 2.7604 - val_mae: 1.1307\n",
      "Epoch 72/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 2.0972 - mae: 0.8873 - val_loss: 8.9492 - val_mae: 1.5739\n",
      "Epoch 73/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - loss: 2.4583 - mae: 0.9587 - val_loss: 2.2471 - val_mae: 1.0289\n",
      "Epoch 74/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - loss: 2.3063 - mae: 0.8408 - val_loss: 4.8496 - val_mae: 1.5768\n",
      "Epoch 75/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 1.9521 - mae: 0.9431 - val_loss: 64.0099 - val_mae: 4.5872\n",
      "Epoch 76/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 152.5204 - mae: 5.7427 - val_loss: 4.2661 - val_mae: 1.4275\n",
      "Epoch 77/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 1.7209 - mae: 0.9131 - val_loss: 2.8445 - val_mae: 1.1763\n",
      "Epoch 78/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 1.2991 - mae: 0.7328 - val_loss: 3.7830 - val_mae: 1.3059\n",
      "Epoch 79/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 1.4571 - mae: 0.7970 - val_loss: 2.1788 - val_mae: 1.0043\n",
      "Epoch 80/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 38.6244 - mae: 1.6299 - val_loss: 29.2488 - val_mae: 3.7553\n",
      "Epoch 81/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 8.8405 - mae: 2.0147 - val_loss: 3.1717 - val_mae: 1.2583\n",
      "Epoch 82/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 1.3469 - mae: 0.8114 - val_loss: 2.5340 - val_mae: 1.1039\n",
      "Epoch 83/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - loss: 1.2265 - mae: 0.7270 - val_loss: 2.2957 - val_mae: 1.0392\n",
      "Epoch 84/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 0.5927 - mae: 0.5599 - val_loss: 2.3138 - val_mae: 1.0469\n",
      "Epoch 85/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 1.4224 - mae: 0.7174 - val_loss: 2.3175 - val_mae: 1.0279\n",
      "Epoch 86/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - loss: 0.5336 - mae: 0.5354 - val_loss: 2.0885 - val_mae: 0.9764\n",
      "Epoch 87/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 1.2517 - mae: 0.6439 - val_loss: 2.3497 - val_mae: 1.0598\n",
      "Epoch 88/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - loss: 0.5473 - mae: 0.5449 - val_loss: 2.0384 - val_mae: 0.9693\n",
      "Epoch 89/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - loss: 0.6474 - mae: 0.5094 - val_loss: 25.3317 - val_mae: 2.9200\n",
      "Epoch 90/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 6.0034 - mae: 1.5286 - val_loss: 4.0188 - val_mae: 1.2023\n",
      "Epoch 91/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 45.0847 - mae: 2.7192 - val_loss: 3.5781 - val_mae: 1.3174\n",
      "Epoch 92/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 1.5230 - mae: 0.8551 - val_loss: 2.6395 - val_mae: 1.0913\n",
      "Epoch 93/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 0.7936 - mae: 0.6312 - val_loss: 2.2477 - val_mae: 1.0137\n",
      "Epoch 94/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - loss: 0.5104 - mae: 0.5223 - val_loss: 2.1673 - val_mae: 0.9848\n",
      "Epoch 95/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 0.4879 - mae: 0.5041 - val_loss: 2.2558 - val_mae: 1.0194\n",
      "Epoch 96/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 1.2297 - mae: 0.6792 - val_loss: 1.9683 - val_mae: 0.9400\n",
      "Epoch 97/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 0.4570 - mae: 0.4790 - val_loss: 46.3673 - val_mae: 3.1247\n",
      "Epoch 98/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 29.0006 - mae: 2.7951 - val_loss: 3.4865 - val_mae: 1.2574\n",
      "Epoch 99/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 5.0056 - mae: 1.2064 - val_loss: 4.5312 - val_mae: 1.5667\n",
      "Epoch 100/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 3.0753 - mae: 1.1543 - val_loss: 3.0448 - val_mae: 1.2093\n",
      "Mean Absolute Error on Test Set: 1.2329527139663696\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Ensure 'e_high' DataFrame is correctly loaded and processed\n",
    "# For example: e_high = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Clean column names\n",
    "e_high.columns = e_high.columns.str.strip()\n",
    "\n",
    "# Print columns to check\n",
    "print(\"Columns in DataFrame:\", e_high.columns)\n",
    "\n",
    "# Define features and target\n",
    "features = ['year', 'parent_entity', 'parent_type', 'commodity', \n",
    "            'product_emissions_MtCO2', 'flaring_emissions_MtCO2', \n",
    "            'venting_emissions_MtCO2', 'total_operational_emissions_MtCO2e']\n",
    "target = 'total_emissions_MtCO2e'\n",
    "\n",
    "# Check if all required columns are present\n",
    "required_columns = features + [target]\n",
    "missing_columns = [col for col in required_columns if col not in e_high.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    print(\"All required columns are present.\")\n",
    "\n",
    "# Drop rows with missing values in features or target\n",
    "e_high = e_high.dropna(subset=required_columns)\n",
    "\n",
    "# Split data into features and target\n",
    "X = e_high[features]\n",
    "y = e_high[target]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing for numeric and categorical features\n",
    "numeric_features = ['product_emissions_MtCO2', 'flaring_emissions_MtCO2', \n",
    "                     'venting_emissions_MtCO2', 'total_operational_emissions_MtCO2e']\n",
    "categorical_features = ['year', 'parent_entity', 'parent_type', 'commodity']\n",
    "\n",
    "# Standardize numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess the training and testing data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train_preprocessed.shape[1], activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear')  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_preprocessed, y_train, epochs=100, batch_size=55, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test_preprocessed, y_test, verbose=0)\n",
    "print(f\"Mean Absolute Error on Test Set: {mae}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# First run, MAE on Test Set is 0.837\n",
    "# Second run, MAE on Test Set is 5.77\n",
    "# Third run, MAE on Test Set is 1.19\n",
    "\n",
    "\n",
    "# Variance is too high, so we will try to lower the amount of features used.\n",
    "previous_features = ['year', 'parent_entity', 'parent_type', 'commodity', \n",
    "            'product_emissions_MtCO2', 'flaring_emissions_MtCO2', \n",
    "            'venting_emissions_MtCO2', 'total_operational_emissions_MtCO2e']\n",
    "updated_features = ['year', 'parent_entity', 'parent_type', 'commodity']\n",
    "\n",
    "# First run, MAE on Test Set is 36.8\n",
    "# Second run, MAE on Test Set is 39.9\n",
    "# Lowering the features made MAE worse. Reset features back to original ones.\n",
    "\n",
    "\n",
    "# First run, MAE on Test Set is 68.66\n",
    "# Second run, MAE on Test Set is -\n",
    "\n",
    "# MAE has increased drastically. Change year to be categorical than numerical.\n",
    "# First run, MAE on Test Set is 40.41\n",
    "# Second run, MAE on Test Set is 39.62\n",
    "\n",
    "# Try adding more layers to the neural network.\n",
    "# First run, MAE on Test Set is 39.51\n",
    "# Second run, MAE on Test Set is 37.39\n",
    "\n",
    "\n",
    "# Lowered batch size from 32 to 16\n",
    "# MAE on Test Set is 36.0\n",
    "\n",
    "# Lower epochs to 50\n",
    "# MAE on Test Set is 44.88\n",
    "\n",
    "# Put layers back to 3 and batch size to 32 and epochs to 150\n",
    "# MAE on Test Set is 39.14\n",
    "\n",
    "# Reset to original model because yikes!\n",
    "# First run, MAE on Test Set is 1.9\n",
    "# Second run, MAE on Test Set is 1.62\n",
    "# Third run, MAE on Test Set is 2.18\n",
    "\n",
    "# Lower to two layers\n",
    "# First run, MAE on Test Set is 1.44\n",
    "# Second run, MAE on Test Set is 1.61\n",
    "# Second run, MAE on Test Set is 1.11\n",
    "\n",
    "# 150 epochs, batch size 48 -> MAE -> 1.24 -> 1.01 -> 1.03\n",
    "# Adding an extra layer brought MAE to 1.42.\n",
    "\n",
    "# 100 epochs, batch size 55 got us under 1.0! Hooray!\n",
    "\n",
    "\n",
    "\n",
    "model.save('carbon_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16f040-7533-4dcb-9989-6d258ab26e39",
   "metadata": {},
   "source": [
    "#### Save the model for Django Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3741d3-2ea5-4501-a45c-9312c0b2f836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
